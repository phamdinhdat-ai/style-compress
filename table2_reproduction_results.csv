Model,Compression Ratio,Method,bertscore
LLaMA-2 13B,0.1,selective-context,0.18718686707174073
LLaMA-2 13B,0.1,LLM-Lingua,0.08421277049355079
LLaMA-2 13B,0.1,vanilla,0.09551220552179938
LLaMA-2 13B,0.1,style-compress,0.049139132310670325
LLaMA-2 13B,0.25,selective-context,0.41073321915015093
LLaMA-2 13B,0.25,LLM-Lingua,0.15823132626213995
LLaMA-2 13B,0.25,vanilla,0.2605466428995841
LLaMA-2 13B,0.25,style-compress,0.229503995175637
LLaMA-2 13B,0.5,selective-context,0.6755327245053272
LLaMA-2 13B,0.5,LLM-Lingua,0.4764379764379765
LLaMA-2 13B,0.5,vanilla,0.4668542839274547
LLaMA-2 13B,0.5,style-compress,0.5827408820299405
GPT-3.5,0.1,selective-context,0.18718686707174073
GPT-3.5,0.1,LLM-Lingua,0.05031446540880503
GPT-3.5,0.1,vanilla,0.10681164055004795
GPT-3.5,0.1,style-compress,0.07354925775978408
GPT-3.5,0.25,selective-context,0.41073321915015093
GPT-3.5,0.25,LLM-Lingua,0.22765678250115642
GPT-3.5,0.25,vanilla,0.22983001658374794
GPT-3.5,0.25,style-compress,0.20019870839542972
GPT-3.5,0.5,selective-context,0.6755327245053272
GPT-3.5,0.5,LLM-Lingua,0.4178909678909679
GPT-3.5,0.5,vanilla,0.464699683877766
GPT-3.5,0.5,style-compress,0.4506472833159152
